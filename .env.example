# Environment Variables for Trellis PM
# Copy this file to .env.local for local development

# ===========================================
# Database Configuration
# ===========================================
# For local PostgreSQL development
DATABASE_URL="postgresql://postgres:postgres@localhost:5432/trellis_pm?schema=public"

# ===========================================
# Application Configuration
# ===========================================
NEXT_PUBLIC_APP_URL="http://localhost:3000"

# ===========================================
# AI Provider Configuration
# ===========================================
# Select which AI provider to use: "openai" | "xai" | "ollama"
# If not set, the first available provider will be used
# AI_PROVIDER="openai"

# -------------------------------------------
# OpenAI Configuration
# -------------------------------------------
# Get your API key from https://platform.openai.com/api-keys
OPENAI_API_KEY=""
# Optional: Override the default model (default: gpt-4o-mini)
# OPENAI_DEFAULT_MODEL="gpt-4o"
# Optional: Organization ID
# OPENAI_ORGANIZATION=""
# Optional: Custom base URL (for Azure OpenAI or proxies)
# OPENAI_BASE_URL="https://api.openai.com/v1"

# -------------------------------------------
# X.AI (Grok) Configuration
# -------------------------------------------
# Get your API key from https://console.x.ai/
XAI_API_KEY=""
# Optional: Override the default model (default: grok-3-fast)
# XAI_DEFAULT_MODEL="grok-3-fast"
# Optional: Custom base URL
# XAI_BASE_URL="https://api.x.ai/v1"

# -------------------------------------------
# Ollama Configuration (Local LLMs)
# -------------------------------------------
# Set to "true" to enable Ollama provider
# OLLAMA_ENABLED="true"
# Ollama server URL (default: http://localhost:11434)
# OLLAMA_BASE_URL="http://localhost:11434"
# Default model (default: llama3.2)
# OLLAMA_DEFAULT_MODEL="llama3.2"
# Request timeout in ms (default: 120000)
# OLLAMA_TIMEOUT="120000"

